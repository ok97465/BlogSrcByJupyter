{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///---\n",
    "layout: post\n",
    "title: \"Numba를 이용한 Cuda 프로그램 예제\"\n",
    "comments: true\n",
    "share: true\n",
    "date: 2019-07-07 12:58:00\n",
    "description: Python에서 Numba를 이용한 Cuda 프로그래밍 예제를 소개한다.\n",
    "tags: python cuda\n",
    "toc: true\n",
    "sitemap :\n",
    "  changefreq : daily\n",
    "  priority : 1.0\n",
    "///---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba를 이용한 Cuda 프로그램 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시간 측정 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show in Markdown\n",
    "class Timer:\n",
    "    \"\"\"클래스 생성 시점부터 소멸 시점까지의 시간을 출력한다.\"\"\"\n",
    "\n",
    "    def __init__(self, func_name: str='this func'):\n",
    "        self.func_name: str = func_name\n",
    "        self.time_start: float = 0.0\n",
    "\n",
    "    def __enter__(self):\n",
    "        import sys\n",
    "        import time\n",
    "        print('{} ==>'.format(self.func_name), end=' ')\n",
    "        sys.stdout.flush()\n",
    "        self.time_start = time.process_time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        import time\n",
    "        time_end = time.process_time()\n",
    "        interval = time_end - self.time_start\n",
    "        print('Elapsed time: {:.8f} sec'.format(interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Info [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 1 CUDA Capable device(s) \n",
      "\n",
      "Device 0: GeForce GTX 850M\n",
      "  Compute Capability: 5.0\n",
      "  Total Memory: 2004 MB\n",
      "  Multiprocessors: 5\n",
      "  CUDA Cores Per Multiprocessor: 128\n",
      "  Total CUDA Cores: 640\n",
      "  ASYNC_ENGINE_COUNT: 1\n",
      "  CAN_MAP_HOST_MEMORY: 1\n",
      "  CLOCK_RATE: 901500\n",
      "  COMPUTE_CAPABILITY_MAJOR: 5\n",
      "  COMPUTE_CAPABILITY_MINOR: 0\n",
      "  COMPUTE_MODE: DEFAULT\n",
      "  CONCURRENT_KERNELS: 1\n",
      "  ECC_ENABLED: 0\n",
      "  GLOBAL_L1_CACHE_SUPPORTED: 0\n",
      "  GLOBAL_MEMORY_BUS_WIDTH: 128\n",
      "  GPU_OVERLAP: 1\n",
      "  INTEGRATED: 0\n",
      "  KERNEL_EXEC_TIMEOUT: 1\n",
      "  L2_CACHE_SIZE: 2097152\n",
      "  LOCAL_L1_CACHE_SUPPORTED: 1\n",
      "  MANAGED_MEMORY: 1\n",
      "  MAXIMUM_SURFACE1D_LAYERED_LAYERS: 2048\n",
      "  MAXIMUM_SURFACE1D_LAYERED_WIDTH: 16384\n",
      "  MAXIMUM_SURFACE1D_WIDTH: 16384\n",
      "  MAXIMUM_SURFACE2D_HEIGHT: 65536\n",
      "  MAXIMUM_SURFACE2D_LAYERED_HEIGHT: 16384\n",
      "  MAXIMUM_SURFACE2D_LAYERED_LAYERS: 2048\n",
      "  MAXIMUM_SURFACE2D_LAYERED_WIDTH: 16384\n",
      "  MAXIMUM_SURFACE2D_WIDTH: 65536\n",
      "  MAXIMUM_SURFACE3D_DEPTH: 4096\n",
      "  MAXIMUM_SURFACE3D_HEIGHT: 4096\n",
      "  MAXIMUM_SURFACE3D_WIDTH: 4096\n",
      "  MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS: 2046\n",
      "  MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH: 16384\n",
      "  MAXIMUM_SURFACECUBEMAP_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE1D_LAYERED_LAYERS: 2048\n",
      "  MAXIMUM_TEXTURE1D_LAYERED_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE1D_LINEAR_WIDTH: 134217728\n",
      "  MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE1D_WIDTH: 65536\n",
      "  MAXIMUM_TEXTURE2D_ARRAY_HEIGHT: 16384\n",
      "  MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES: 2048\n",
      "  MAXIMUM_TEXTURE2D_ARRAY_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE2D_GATHER_HEIGHT: 16384\n",
      "  MAXIMUM_TEXTURE2D_GATHER_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE2D_HEIGHT: 65536\n",
      "  MAXIMUM_TEXTURE2D_LINEAR_HEIGHT: 65536\n",
      "  MAXIMUM_TEXTURE2D_LINEAR_PITCH: 1048544\n",
      "  MAXIMUM_TEXTURE2D_LINEAR_WIDTH: 65536\n",
      "  MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: 16384\n",
      "  MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE2D_WIDTH: 65536\n",
      "  MAXIMUM_TEXTURE3D_DEPTH: 4096\n",
      "  MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE: 16384\n",
      "  MAXIMUM_TEXTURE3D_HEIGHT: 4096\n",
      "  MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE: 2048\n",
      "  MAXIMUM_TEXTURE3D_WIDTH: 4096\n",
      "  MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE: 2048\n",
      "  MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS: 2046\n",
      "  MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURECUBEMAP_WIDTH: 16384\n",
      "  MAX_BLOCK_DIM_X: 1024\n",
      "  MAX_BLOCK_DIM_Y: 1024\n",
      "  MAX_BLOCK_DIM_Z: 64\n",
      "  MAX_GRID_DIM_X: 2147483647\n",
      "  MAX_GRID_DIM_Y: 65535\n",
      "  MAX_GRID_DIM_Z: 65535\n",
      "  MAX_PITCH: 2147483647\n",
      "  MAX_REGISTERS_PER_BLOCK: 65536\n",
      "  MAX_REGISTERS_PER_MULTIPROCESSOR: 65536\n",
      "  MAX_SHARED_MEMORY_PER_BLOCK: 49152\n",
      "  MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 65536\n",
      "  MAX_THREADS_PER_BLOCK: 1024\n",
      "  MAX_THREADS_PER_MULTIPROCESSOR: 2048\n",
      "  MEMORY_CLOCK_RATE: 1001000\n",
      "  MULTI_GPU_BOARD: 0\n",
      "  MULTI_GPU_BOARD_GROUP_ID: 0\n",
      "  PCI_BUS_ID: 1\n",
      "  PCI_DEVICE_ID: 0\n",
      "  PCI_DOMAIN_ID: 0\n",
      "  STREAM_PRIORITIES_SUPPORTED: 1\n",
      "  SURFACE_ALIGNMENT: 512\n",
      "  TCC_DRIVER: 0\n",
      "  TEXTURE_ALIGNMENT: 512\n",
      "  TEXTURE_PITCH_ALIGNMENT: 32\n",
      "  TOTAL_CONSTANT_MEMORY: 65536\n",
      "  UNIFIED_ADDRESSING: 1\n",
      "  WARP_SIZE: 32\n"
     ]
    }
   ],
   "source": [
    "# Show in Markdown\n",
    "import pycuda.driver as drv\n",
    "drv.init()\n",
    "\n",
    "print('Detected {} CUDA Capable device(s) \\n'.format(drv.Device.count()))\n",
    "\n",
    "for i in range(drv.Device.count()):\n",
    "\n",
    "    dev = drv.Device(i)\n",
    "    print(f'Device {i}: {dev.name()}')\n",
    "    compute_capability = float('%d.%d' % dev.compute_capability())\n",
    "    print(f'  Compute Capability: {compute_capability}')\n",
    "    print(f'  Total Memory: {dev.total_memory() // (1024**2)} MB')\n",
    "\n",
    "    dev_attr_tuples = dev.get_attributes().items()\n",
    "    dev_attributes = {}\n",
    "\n",
    "    for k, v in dev_attr_tuples:\n",
    "        dev_attributes[str(k)] = v\n",
    "\n",
    "    num_mp = dev_attributes['MULTIPROCESSOR_COUNT']\n",
    "\n",
    "    cuda_cores_per_mp = {5.0 : 128,\n",
    "                         5.1 : 128,\n",
    "                         5.2 : 128,\n",
    "                         6.0 : 64,\n",
    "                         6.1 : 128,\n",
    "                         6.2 : 128}[compute_capability]\n",
    "    print(f'  Multiprocessors: {num_mp}')\n",
    "    print(f'  CUDA Cores Per Multiprocessor: {cuda_cores_per_mp}')\n",
    "    print(f'  Total CUDA Cores: {num_mp * cuda_cores_per_mp}')\n",
    "\n",
    "    dev_attributes.pop('MULTIPROCESSOR_COUNT')\n",
    "\n",
    "    for k in dev_attributes.keys():\n",
    "        print(f'  {k}: {dev_attributes[k]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum\n",
    "\n",
    "GPU는 Code 최적화에 따라서 연산 시간에 많은 차이를 보인다. 간단한 SUM Code 조차도 CUDA에서 최적화 하는 것이 쉽지 않은 일이다([GPU_SUM.pdf](/assets/data/Numba_Cuda/gpu_sum_reduction.pdf)). 하지만 Numba의 reduce를 이용하면 SUM을 CUDA로 쉽게 Coding 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Kernel Code using reduce of numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show in Markdown\n",
    "from numba import cuda\n",
    "from numba.cuda import to_device\n",
    "import numpy as np\n",
    "\n",
    "@cuda.reduce\n",
    "def sum_reduce(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Host Code using reduce of numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum By CPU ==> Elapsed time: 0.21539920 sec\n",
      "Sum By GPU ==> Elapsed time: 0.02144496 sec\n",
      "CPU Result: 2670.03662109375\n",
      "GPU Result: 2670.000244140625\n"
     ]
    }
   ],
   "source": [
    "# Show in Markdown\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import float32\n",
    "from numpy.random_intel import standard_normal\n",
    "\n",
    "n_sample = 2 ** 27 - 157\n",
    "\n",
    "data = float32(standard_normal(n_sample))\n",
    "d_data = to_device(data.copy())\n",
    "\n",
    "with Timer('Sum By CPU'):\n",
    "    sum_cpu = np.sum(data)\n",
    "\n",
    "_ = sum_reduce(data)\n",
    "    \n",
    "with Timer('Sum By GPU'):\n",
    "    sum_gpu = sum_reduce(d_data)\n",
    "    \n",
    "print(f'CPU Result: {sum_cpu}')\n",
    "print(f'GPU Result: {sum_gpu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Code using reduction\n",
    "\n",
    "[GPU_SUM.pdf](/assets/data/Numba_Cuda/gpu_sum_reduction.pdf)의 예제를 Numba에서도 구현 해 볼 수 있다. 아래 Sum_kernel을 한번만 호출하고 남은 연산을 CPU에서 처리 하여도 GPU를 활용하는 것이 더 효율적임을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum By CPU ==> Elapsed time: 0.21521380 sec\n",
      "Sum By GPU ==> Elapsed time: 0.08513450 sec\n",
      "CPU Result: -6470.31591796875\n",
      "GPU Result: -6470.2724609375\n"
     ]
    }
   ],
   "source": [
    "# Show in Markdown\n",
    "# Kernel Code\n",
    "from numba import cuda\n",
    "from numba.cuda import to_device\n",
    "import numpy as np\n",
    "\n",
    "@cuda.jit(\"void(float32[:], float32[:], int64)\")\n",
    "def sum_kernel(out, data, n):\n",
    "    tid = cuda.threadIdx.x\n",
    "    idx = cuda.grid(1)\n",
    "    idx_block_start = cuda.blockIdx.x * cuda.blockDim.x\n",
    "\n",
    "    if idx >= n:\n",
    "        return\n",
    "\n",
    "    stride = cuda.blockDim.x // 2\n",
    "\n",
    "    while stride > 0:\n",
    "        if tid < stride and (idx_block_start + tid + stride) < n:\n",
    "            data[idx_block_start + tid] += data[idx_block_start + tid + stride]\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "\n",
    "    if (tid == 0):\n",
    "        out[cuda.blockIdx.x] = data[idx_block_start]\n",
    "\n",
    "# Host Code\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import float32\n",
    "from numpy.random_intel import standard_normal\n",
    "\n",
    "n_sample = 2 ** 27 - 157\n",
    "\n",
    "data = float32(standard_normal(n_sample))\n",
    "d_data = to_device(data.copy())\n",
    "\n",
    "with Timer('Sum By CPU'):\n",
    "    sum_cpu = np.sum(data)\n",
    "\n",
    "threadsperblock = 128\n",
    "blockspergrid = math.ceil(n_sample / threadsperblock)\n",
    "out_of_gpu = cuda.device_array(blockspergrid, float32)\n",
    "\n",
    "sum_kernel[blockspergrid, threadsperblock](out_of_gpu, data, n_sample) # For Compile\n",
    "\n",
    "with Timer('Sum By GPU'):\n",
    "    sum_kernel[blockspergrid, threadsperblock](out_of_gpu, d_data, n_sample)\n",
    "    out_from_gpu = out_of_gpu.copy_to_host()\n",
    "    sum_gpu = np.sum(out_from_gpu)\n",
    "\n",
    "print(f'CPU Result: {sum_cpu}')\n",
    "print(f'GPU Result: {sum_gpu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료\n",
    "[1] Dr. Brian Tuomanen. (2018). Chapter3, Hands-On GPU Programming with Python and CUDA (39)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
