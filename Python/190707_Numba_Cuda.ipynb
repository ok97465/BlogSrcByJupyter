{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///---\n",
    "layout: post\n",
    "title: \"Numba를 이용한 Cuda 프로그램 예제\"\n",
    "comments: true\n",
    "share: true\n",
    "date: 2019-07-07 12:58:00\n",
    "description: Python에서 Numba를 이용한 Cuda 프로그래밍 예제를 소개한다.\n",
    "tags: python cuda\n",
    "toc: true\n",
    "sitemap :\n",
    "  changefreq : daily\n",
    "  priority : 1.0\n",
    "///---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba를 이용한 Cuda 프로그램 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda 정리[1]\n",
    "\n",
    "이 절에서는 간단한 Cuda 프로그래밍 내용을 정리한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory hierarchy\n",
    "\n",
    "<figure>\n",
    "    <img src='../assets/images/Cuda/memory_hierarchy.png' alt='Memory hierarchy' width=\"640\" />\n",
    "    <figcaption class=\"figure-caption\">Memory hierarchy</figcaption>\n",
    "</figure>\n",
    "\n",
    "- Registers\n",
    "  - Thread 간에도 공유되지 않는다.\n",
    "  - 함수에서 선언한 변수를 저장한다.\n",
    "  - 함수에서 선언한 변수의 크기가 Thread의 Register개수를 넘을 경우 Local 메모리에 위치한다.\n",
    "- Shared memory\n",
    "  - \\__shared\\__를 이용하여 선언한다.\n",
    "  - Local, Global memory보다 지연시간이 적다.\n",
    "  - Block내의 Thread간 공유된다.\n",
    "  - \\__syncThreads()으로 Shared memory의 Coherent를 유지한다.\n",
    "  - L1 cache와 하드웨어를 공유하고 할당량은 cudaFuncSetCache함수를 이용하여 설정가능한다.\n",
    "- Local memory\n",
    "  - Registers의 공간 부족으로 Cache에 위치한 변수\n",
    "- Constant memory\n",
    "  - \\__constant\\__를 이용하여 선언한다.\n",
    "  - Global scope 선언되어야 한다.\n",
    "  - Kernel 실행 전 cudaMemcpyToSymbo로 값을 설정하여야한다.\n",
    "- Texture memory\n",
    "  - Read Only\n",
    "  - 2D Spatial Locality에 최적회 되어 있다.\n",
    "- Global memory\n",
    "  - \\__device\\__식별자를 이용하여 선언하거나 Host에서 cudaMalloc을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  \n",
    "---\n",
    "\n",
    "- CUDA Variable and Type Qualifier\n",
    "\n",
    "|   Qualifier  |  Variable Name |  Memory  |  Scope |   Lifespan  |\n",
    "|:------------:|:--------------:|:--------:|:------:|:-----------:|\n",
    "|              |    float var   | Register | Thread |    Thread   |\n",
    "|              | float var[100] |   Local  | Thread |    Thread   |\n",
    "|  __shared__  |    float var   |  Shared  |  Block |    Block    |\n",
    "|  __device__  |    float var   |  Global  | Global | Application |\n",
    "| __constant__ |    float var   | Constant | Global | Application |\n",
    "\n",
    "- Salient Features of Device Memory\n",
    "\n",
    "|  Memory  | On/Off Chip | Cached  | Access |        Scope         |     Lifetime    |\n",
    "|:--------:|:-----------:|:-------:|:------:|:--------------------:|:---------------:|\n",
    "| Register |      On     |   n/a   |   R/W  |       1 thread       |      Thread     |\n",
    "|   Local  |     Off     |   Yes   |   R/W  |       1 thread       |      Thread     |\n",
    "|  Shared  |      On     |   n/a   |   R/W  | All threads in block |      Block      |\n",
    "|  Global  |     Off     |    †    |   R/W  |  All threads + host  | Host allocation |\n",
    "| Constant |     Off     |   Yes   |    R   |  All threads + host  | Host allocation |\n",
    "|  Texture |     Off     |   Yes   |    R   |  All threads + host  | Host allocation |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host <-> Device\n",
    "Host와 device의 변수가 동일한 파일에 선언되어 있어도 직접적인 참조는 불가능한다. 배열의 경우 cudaAlloc으로 할당된 포인터와 cudaMemcpy를 이용하여 Host와 device간 데이터를 교환할 수 있다. 하지만 Global scope에 __device__로 선언된 변수의 경우는 변수의 주소값과 cudaMemcpy를 이용하여 값을 전송 할 수 없다. 이런 경우에는 cudaGetSymboAddress를 이용하여 포인터를 얻어와서 cudaMemcpy를 이용하거나 cudaMemcpyToSymbol 명령을 이용하여 한다.\n",
    "\n",
    "```c\n",
    "#include <cuda_runtime.h>\n",
    "#include <stdio.h>\n",
    "__device__ float devData;\n",
    "__global__ void checkGlobalVariable() {\n",
    "   devData +=2.0f;\n",
    "}\n",
    "int main(void) {\n",
    "   float value = 3.14f;\n",
    "   cudaMemcpyToSymbol(devData, &value, sizeof(float));\n",
    "   // 위의 처럼 devData에 저장하거나\n",
    "   float *dptr = NULL;\n",
    "   cudaGetSymbolAddress((void**)&dptr, devData);\n",
    "   cudaMemcpy(dptr, &value, sizeof(float), cudaMemcpyHostToDevice);\n",
    "   //\n",
    "   checkGlobalVariable <<<1, 1>>>();\n",
    "   cudaMemcpyFromSymbol(&value, devData, sizeof(float));\n",
    "   cudaDeviceReset();\n",
    "   return EXIT_SUCCESS;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinned Memory\n",
    "\n",
    "CPU에서 선언한 배열의 경우 Pageable 메모리 형태로 생성되므로 Device로 데이터를 전송 할때 Overhead가 생성된다. Host에서 사용할 메모리를 cudaMallocHost/cudaFreeHost를 이용하면 Pinned Memory(Non-pageable)로 선언되므로 Overhead를 줄일 수 있다. Pinned Memory가 많을 수록 host의 전체 성능이 저하 될 수 있으므로 시스템과 프로그램 상황에 맞게 조절되어야 한다.\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <img src='../assets/images/Cuda/pinned_memroy.png' alt='Pinned Memory' width=\"641\" />\n",
    "    <figcaption class=\"figure-caption\">Pinned Memory</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Copy Memory\n",
    "\n",
    "일반적으로는 Host<->Device간의 데이터를 직접 주고 받을 수 없지만 Zero-Copy memory로 선언된 영역에 대해서는 Host와 device가 접근할 수 있다. cudaHostAlloc/cudaFreeHost를 이용하여 선언/해제 할 수 있다.\n",
    "\n",
    "Zero-Copy Memory의 장점은 다음과 같다.\n",
    "\n",
    "- Leveraging host memory when there is insufficient device memory\n",
    "- Avoiding explicit data transfer between the host and device\n",
    "- Improving PCIe transfer rates\n",
    "\n",
    "큰 배열의 경우 cudaMalloc을 사용하는 것이 훨씬 효율적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시간 측정 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show in Markdown\n",
    "class Timer:\n",
    "    \"\"\"클래스 생성 시점부터 소멸 시점까지의 시간을 출력한다.\"\"\"\n",
    "\n",
    "    def __init__(self, func_name: str='this func'):\n",
    "        self.func_name: str = func_name\n",
    "        self.time_start: float = 0.0\n",
    "\n",
    "    def __enter__(self):\n",
    "        import sys\n",
    "        import time\n",
    "        print(f'{self.func_name} ==>', end=' ')\n",
    "        sys.stdout.flush()\n",
    "        self.time_start = time.perf_counter_ns()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        import time\n",
    "        time_end = time.perf_counter_ns()\n",
    "        interval = (time_end - self.time_start) / 1e9\n",
    "        print(f'Elapsed time: {interval:.8f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Info [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 1 CUDA Capable device(s) \n",
      "\n",
      "Device 0: GeForce GTX 850M\n",
      "  Compute Capability: 5.0\n",
      "  Total Memory: 2004 MB\n",
      "  Multiprocessors: 5\n",
      "  CUDA Cores Per Multiprocessor: 128\n",
      "  Total CUDA Cores: 640\n",
      "  ASYNC_ENGINE_COUNT: 1\n",
      "  CAN_MAP_HOST_MEMORY: 1\n",
      "  CLOCK_RATE: 901500\n",
      "  COMPUTE_CAPABILITY_MAJOR: 5\n",
      "  COMPUTE_CAPABILITY_MINOR: 0\n",
      "  COMPUTE_MODE: DEFAULT\n",
      "  CONCURRENT_KERNELS: 1\n",
      "  ECC_ENABLED: 0\n",
      "  GLOBAL_L1_CACHE_SUPPORTED: 0\n",
      "  GLOBAL_MEMORY_BUS_WIDTH: 128\n",
      "  GPU_OVERLAP: 1\n",
      "  INTEGRATED: 0\n",
      "  KERNEL_EXEC_TIMEOUT: 1\n",
      "  L2_CACHE_SIZE: 2097152\n",
      "  LOCAL_L1_CACHE_SUPPORTED: 1\n",
      "  MANAGED_MEMORY: 1\n",
      "  MAXIMUM_SURFACE1D_LAYERED_LAYERS: 2048\n",
      "  MAXIMUM_SURFACE1D_LAYERED_WIDTH: 16384\n",
      "  MAXIMUM_SURFACE1D_WIDTH: 16384\n",
      "  MAXIMUM_SURFACE2D_HEIGHT: 65536\n",
      "  MAXIMUM_SURFACE2D_LAYERED_HEIGHT: 16384\n",
      "  MAXIMUM_SURFACE2D_LAYERED_LAYERS: 2048\n",
      "  MAXIMUM_SURFACE2D_LAYERED_WIDTH: 16384\n",
      "  MAXIMUM_SURFACE2D_WIDTH: 65536\n",
      "  MAXIMUM_SURFACE3D_DEPTH: 4096\n",
      "  MAXIMUM_SURFACE3D_HEIGHT: 4096\n",
      "  MAXIMUM_SURFACE3D_WIDTH: 4096\n",
      "  MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS: 2046\n",
      "  MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH: 16384\n",
      "  MAXIMUM_SURFACECUBEMAP_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE1D_LAYERED_LAYERS: 2048\n",
      "  MAXIMUM_TEXTURE1D_LAYERED_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE1D_LINEAR_WIDTH: 134217728\n",
      "  MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE1D_WIDTH: 65536\n",
      "  MAXIMUM_TEXTURE2D_ARRAY_HEIGHT: 16384\n",
      "  MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES: 2048\n",
      "  MAXIMUM_TEXTURE2D_ARRAY_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE2D_GATHER_HEIGHT: 16384\n",
      "  MAXIMUM_TEXTURE2D_GATHER_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE2D_HEIGHT: 65536\n",
      "  MAXIMUM_TEXTURE2D_LINEAR_HEIGHT: 65536\n",
      "  MAXIMUM_TEXTURE2D_LINEAR_PITCH: 1048544\n",
      "  MAXIMUM_TEXTURE2D_LINEAR_WIDTH: 65536\n",
      "  MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: 16384\n",
      "  MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE2D_WIDTH: 65536\n",
      "  MAXIMUM_TEXTURE3D_DEPTH: 4096\n",
      "  MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE: 16384\n",
      "  MAXIMUM_TEXTURE3D_HEIGHT: 4096\n",
      "  MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE: 2048\n",
      "  MAXIMUM_TEXTURE3D_WIDTH: 4096\n",
      "  MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE: 2048\n",
      "  MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS: 2046\n",
      "  MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURECUBEMAP_WIDTH: 16384\n",
      "  MAX_BLOCK_DIM_X: 1024\n",
      "  MAX_BLOCK_DIM_Y: 1024\n",
      "  MAX_BLOCK_DIM_Z: 64\n",
      "  MAX_GRID_DIM_X: 2147483647\n",
      "  MAX_GRID_DIM_Y: 65535\n",
      "  MAX_GRID_DIM_Z: 65535\n",
      "  MAX_PITCH: 2147483647\n",
      "  MAX_REGISTERS_PER_BLOCK: 65536\n",
      "  MAX_REGISTERS_PER_MULTIPROCESSOR: 65536\n",
      "  MAX_SHARED_MEMORY_PER_BLOCK: 49152\n",
      "  MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 65536\n",
      "  MAX_THREADS_PER_BLOCK: 1024\n",
      "  MAX_THREADS_PER_MULTIPROCESSOR: 2048\n",
      "  MEMORY_CLOCK_RATE: 1001000\n",
      "  MULTI_GPU_BOARD: 0\n",
      "  MULTI_GPU_BOARD_GROUP_ID: 0\n",
      "  PCI_BUS_ID: 1\n",
      "  PCI_DEVICE_ID: 0\n",
      "  PCI_DOMAIN_ID: 0\n",
      "  STREAM_PRIORITIES_SUPPORTED: 1\n",
      "  SURFACE_ALIGNMENT: 512\n",
      "  TCC_DRIVER: 0\n",
      "  TEXTURE_ALIGNMENT: 512\n",
      "  TEXTURE_PITCH_ALIGNMENT: 32\n",
      "  TOTAL_CONSTANT_MEMORY: 65536\n",
      "  UNIFIED_ADDRESSING: 1\n",
      "  WARP_SIZE: 32\n"
     ]
    }
   ],
   "source": [
    "# Show in Markdown\n",
    "import pycuda.driver as drv\n",
    "drv.init()\n",
    "\n",
    "print('Detected {} CUDA Capable device(s) \\n'.format(drv.Device.count()))\n",
    "\n",
    "for i in range(drv.Device.count()):\n",
    "\n",
    "    dev = drv.Device(i)\n",
    "    print(f'Device {i}: {dev.name()}')\n",
    "    compute_capability = float('%d.%d' % dev.compute_capability())\n",
    "    print(f'  Compute Capability: {compute_capability}')\n",
    "    print(f'  Total Memory: {dev.total_memory() // (1024**2)} MB')\n",
    "\n",
    "    dev_attr_tuples = dev.get_attributes().items()\n",
    "    dev_attributes = {}\n",
    "\n",
    "    for k, v in dev_attr_tuples:\n",
    "        dev_attributes[str(k)] = v\n",
    "\n",
    "    num_mp = dev_attributes['MULTIPROCESSOR_COUNT']\n",
    "\n",
    "    cuda_cores_per_mp = {5.0 : 128,\n",
    "                         5.1 : 128,\n",
    "                         5.2 : 128,\n",
    "                         6.0 : 64,\n",
    "                         6.1 : 128,\n",
    "                         6.2 : 128}[compute_capability]\n",
    "    print(f'  Multiprocessors: {num_mp}')\n",
    "    print(f'  CUDA Cores Per Multiprocessor: {cuda_cores_per_mp}')\n",
    "    print(f'  Total CUDA Cores: {num_mp * cuda_cores_per_mp}')\n",
    "\n",
    "    dev_attributes.pop('MULTIPROCESSOR_COUNT')\n",
    "\n",
    "    for k in dev_attributes.keys():\n",
    "        print(f'  {k}: {dev_attributes[k]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum\n",
    "\n",
    "GPU는 Code 최적화에 따라서 연산 시간에 많은 차이를 보인다. 간단한 SUM Code 조차도 CUDA에서 최적화 하는 것이 쉽지 않은 일이다([GPU_SUM.pdf](/assets/data/Numba_Cuda/gpu_sum_reduction.pdf)). 하지만 Numba의 reduce를 이용하면 SUM을 CUDA로 쉽게 Coding 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum Kernel Code using reduce of numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show in Markdown\n",
    "from numba import cuda\n",
    "from numba.cuda import to_device\n",
    "import numpy as np\n",
    "\n",
    "@cuda.reduce\n",
    "def sum_reduce(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum Host Code using reduce of numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum By CPU ==> Elapsed time: 0.02662821 sec\n",
      "Sum By GPU ==> Elapsed time: 0.01087535 sec\n",
      "CPU Result: 4220.08349609375\n",
      "GPU Result: 4220.095703125\n"
     ]
    }
   ],
   "source": [
    "# Show in Markdown\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import float32\n",
    "from mkl_random import standard_normal\n",
    "\n",
    "n_sample = 2 ** 26\n",
    "\n",
    "data = float32(standard_normal(n_sample))\n",
    "d_data = to_device(data.copy())\n",
    "\n",
    "with Timer('Sum By CPU'):\n",
    "    sum_cpu = np.sum(data)\n",
    "\n",
    "_ = sum_reduce(data)\n",
    "    \n",
    "with Timer('Sum By GPU'):\n",
    "    sum_gpu = sum_reduce(d_data)\n",
    "    \n",
    "print(f'CPU Result: {sum_cpu}')\n",
    "print(f'GPU Result: {sum_gpu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum Code using reduction\n",
    "\n",
    "[GPU_SUM.pdf](/assets/data/Numba_Cuda/gpu_sum_reduction.pdf)의 예제를 Numba에서도 구현 해 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum By CPU ==> Elapsed time: 0.02695911 sec\n",
      "Sum By GPU ==> Elapsed time: 0.05354665 sec\n",
      "CPU Result: 458.8485107421875\n",
      "GPU Result: 458.8516845703125\n"
     ]
    }
   ],
   "source": [
    "# Show in Markdown\n",
    "# Kernel Code\n",
    "from numba import cuda\n",
    "from numba.cuda import to_device\n",
    "import numpy as np\n",
    "\n",
    "@cuda.jit(\"void(float32[:], float32[:], int64)\")\n",
    "def sum_kernel(out, data, n):\n",
    "    tid = cuda.threadIdx.x\n",
    "    idx = cuda.grid(1)\n",
    "    idx_block_start = cuda.blockIdx.x * cuda.blockDim.x\n",
    "\n",
    "    if idx >= n:\n",
    "        return\n",
    "\n",
    "    stride = cuda.blockDim.x // 2\n",
    "\n",
    "    while stride > 0:\n",
    "        if tid < stride and (idx_block_start + tid + stride) < n:\n",
    "            data[idx_block_start + tid] += data[idx_block_start + tid + stride]\n",
    "        cuda.syncthreads()\n",
    "        stride //= 2\n",
    "\n",
    "    if (tid == 0):\n",
    "        out[cuda.blockIdx.x] = data[idx_block_start]\n",
    "\n",
    "# Host Code\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import float32\n",
    "from mkl_random import standard_normal\n",
    "\n",
    "n_sample = 2 ** 26\n",
    "\n",
    "data = float32(standard_normal(n_sample))\n",
    "d_data = to_device(data.copy())\n",
    "\n",
    "with Timer('Sum By CPU'):\n",
    "    sum_cpu = np.sum(data)\n",
    "\n",
    "threadsperblock = 128\n",
    "blockspergrid = math.ceil(n_sample / threadsperblock)\n",
    "out_of_gpu = cuda.device_array(blockspergrid, float32)\n",
    "\n",
    "sum_kernel[blockspergrid, threadsperblock](out_of_gpu, data, n_sample) # For Compile\n",
    "\n",
    "with Timer('Sum By GPU'):\n",
    "    sum_kernel[blockspergrid, threadsperblock](out_of_gpu, d_data, n_sample)\n",
    "    out_from_gpu = out_of_gpu.copy_to_host()\n",
    "    sum_gpu = np.sum(out_from_gpu)\n",
    "\n",
    "print(f'CPU Result: {sum_cpu}')\n",
    "print(f'GPU Result: {sum_gpu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료\n",
    "[1] Professional CUDA C Programming  \n",
    "[2] Dr. Brian Tuomanen. (2018). Chapter3, Hands-On GPU Programming with Python and CUDA (39)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
